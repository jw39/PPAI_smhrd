{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8158b46c-5714-4921-8e97-ab6b485eee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c7ca2f3-3ee0-4034-96dd-edef8742cb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp949 인코딩 오류 발생\n"
     ]
    }
   ],
   "source": [
    "# data = pd.read_csv('./data/original_reviews.csv', index_col = 'review_idx')\n",
    "# data = pd.read_csv('./data/result_review.csv', index_col='review_idx', encoding='euc-kr')\n",
    "# UTF-8 인코딩 시도\n",
    "# try:\n",
    "#     data = pd.read_csv('./data/result_review.csv', index_col='review_idx', encoding='utf-8')\n",
    "# except UnicodeDecodeError:\n",
    "#     print(\"UTF-8 인코딩 오류 발생\")\n",
    "\n",
    "# cp949 인코딩 시도 (일부 한국어 파일에서 사용됨)\n",
    "try:\n",
    "    data = pd.read_csv('./data/result_review.csv', index_col='review_idx', encoding='cp949')\n",
    "except UnicodeDecodeError:\n",
    "    print(\"cp949 인코딩 오류 발생\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2208602c-5935-4c3e-b19a-1823424498e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a306d212-4113-4079-bfa7-d6edaeb4e71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['원더 세라마이드 모찌 토너', '1025 독도 토너', '서플 프레퍼레이션 페이셜 토너', '오 떼르말',\n",
       "       '캐롯 카로틴 카밍 워터 패드', '로열허니 프로폴리스 인리치 에센스', '프로폴리스 에너지 앰플 미스트',\n",
       "       '더 트루 크림 아쿠아 밤', '블루베리 리밸런싱 스킨', '익스트림 크림', '블랙 스네일 크림',\n",
       "       '데일리 모이스쳐 테라피 페이셜 크림', '더 트루 크림 모이스춰라이징 밤', '순정 약산성 5.5 진정 토너',\n",
       "       '다이브인 저분자 히알루론산 세럼', '파워 10 포뮬라 엘아이 이펙터 감초줄렌', '하또무기 스킨 컨디셔너',\n",
       "       '웰빙 녹차 스킨', '윌 프로디쥬스 멀티 드라이 오일', '비폴렌 리뉴 앰풀러', '노세범 미네랄 파우더',\n",
       "       '더블 웨어 스테이 인 플레이스 메이크업 [SPF10/PA++]',\n",
       "       '잉크래스팅 파운데이션 슬림핏 이엑스 [SPF30/PA++]', '핏미 컨실러',\n",
       "       '에센셜 스킨 누더 쿠션 [SPF50+/PA+++]', '래디언트 크리미 컨실러', '더 포어페셔널',\n",
       "       '퓨처리스트 아쿠아 브릴리언스 파운데이션 [SPF20/PA+++]', '디파이닝 커버 컨실러',\n",
       "       '블랙 쿠션 [SPF34/PA++]', '에어 코튼 메이크업 베이스 [SPF30/PA++]',\n",
       "       '래스팅 실크 UV 파운데이션 [SPF20]', '피치뽀송 멀티 피니시 파우더',\n",
       "       '퍼펙팅 래스트 파운데이션 [SPF30/PA++]', 'UHD 프레스드 파우더', '기름종이 파우더',\n",
       "       '올데이 타이트 메이크업 세팅 픽서', '올 나이터 메이크업 세팅 스프레이', '제로 쿠션 [SPF20/PA++]',\n",
       "       '라스트 벨벳 틴트', '레트로 매트 립스틱', '루쥬 쀠르 꾸뛰르 베르니 아 레브르', '차차틴트',\n",
       "       '모이스처라이징 립밤 클래식', '아토덤 립스틱', '스틱레브르 오리지널', '체크 글로시 블라스터 틴트',\n",
       "       '모이스처라이징 립밤 클래식 튜브', '립밤', '쥬시 래스팅 틴트', '딜라이트 토니틴트',\n",
       "       '진저 슈가 오버나이트 립 마스크', '무드 라이어 벨벳 틴트', '루쥬 볼륍떼 샤인', '벨벳 립 틴트',\n",
       "       '디어 달링 워터젤 틴트', '립테라피 오리지널', '따뚜아쥬 꾸뛰르', '룩 앳 마이 아이즈 [카페]',\n",
       "       '히로인메이크 롱앤컬 마스카라 EX', '히로인메이크 스무스 리퀴드 아이라이너 N', '룩 앳 마이 아이즈',\n",
       "       '하드 포뮬라', '매트 아이 컬러', '킬래쉬 수퍼프루프 마스카라 [롱 컬링]', '헤비로테이션 컬러링 아이브로우',\n",
       "       '잉크 블랙 카라 AD [롱래시 컬링]', '매그니피센트 메탈 글리터 & 글로우 리퀴드 아이섀도우',\n",
       "       '섀이드 앤 섀도우', '아이돌 리얼 래쉬 픽서', '스키니 꼼꼼카라', '백젤 아이라이너', '모노 아이즈 [매트]',\n",
       "       '아이섀도우', '닥터마스카라 픽서 포 슈퍼 롱래쉬', '아트클래스 바이 로댕 쉐딩', '치크 팝', '파스텔 블러셔',\n",
       "       '블러쉬', '크리스탈 블러셔', '라스트 블러쉬 [쉐딩]', '러블리 쿠키 블러셔', '내츄럴 치크N',\n",
       "       '샘물 싱글 블러셔', '무드 레시피 페이스 블러쉬', '코튼 블러셔', '코튼 컨투어', '프리즘 하이라이터',\n",
       "       '미네랄라이즈 스킨피니쉬', '그림자 쉐딩', '슈가볼 벨벳 블러셔', '글로우 플로어 치크', '미네랄라이즈 블러쉬',\n",
       "       '매트 래디언스 베이크드 파우더 하이라이터', '샘물 스마일 베베 블러셔', '고급 파우더 브러시 (대)',\n",
       "       '아이래쉬 컬러', '블렌딩 퍼프', '우루우루 화장솜', '핑크팝 브러시', '잘 스며드는 화장솜', '물방울퍼프',\n",
       "       '소프트 아이래쉬 컬', '블러쉬 브러쉬', '소프트 5겹 화장솜', '에어쿠션® 전용 퍼프', 'EBR 눈썹칼',\n",
       "       '미라클 컴플렉션 스펀지', '스키니 픽스 블렌더', '퍼펙트 아이래쉬 컬러', '파우더 오일크리어 페이퍼',\n",
       "       '실속형비후라', '텐션 팩트 퍼프 [밀착]', '블렌딩총알브러시', '힐링 티 가든 그린티 클렌징 워터',\n",
       "       '녹차 필링젤', '힐링 티 가든 티트리 클렌징 워터', '미네랄 립앤아이 메이크업 리무버', '센시비오 H2O',\n",
       "       '퓨어 클렌징 오일', '페이셜 마일드 필링', '클린 잇 제로 클렌징 밤 오리지널',\n",
       "       '클리어 훼이스 스파 립앤아이 메이크업 리무버', '셀 리뉴 바이오 마이크로 필 소프트 젤',\n",
       "       '올리브 리얼 클렌징 티슈', '센시티브 패드', '티스 딥 오프 클렌징 오일', '브라이트닝 필링 젤',\n",
       "       '순행클렌징오일', '스웨덴 에그팩 라놀린 앤 로즈워터', '세이프 미 릴리프 모이스처 클렌징 폼',\n",
       "       '프레쉬 모이스춰 립앤아이 리무버', '살구씨 코 블랙헤드 클렌징 오일', '순행클렌징폼'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cos_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87747039-f1dc-43cd-b094-0178412dac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4952eb7a-c171-4a76-8119-0bbb87a0af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3a525c5e-2140-4fae-8fb7-a427880f951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "311eb62e-d1ba-4246-bc83-a9a52fd5ec35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EC': '연결 어미',\n",
       " 'ECD': '의존적 연결 어미',\n",
       " 'ECE': '대등 연결 어미',\n",
       " 'ECS': '보조적 연결 어미',\n",
       " 'EF': '종결 어미',\n",
       " 'EFA': '청유형 종결 어미',\n",
       " 'EFI': '감탄형 종결 어미',\n",
       " 'EFN': '평서형 종결 어미',\n",
       " 'EFO': '명령형 종결 어미',\n",
       " 'EFQ': '의문형 종결 어미',\n",
       " 'EFR': '존칭형 종결 어미',\n",
       " 'EP': '선어말 어미',\n",
       " 'EPH': '존칭 선어말 어미',\n",
       " 'EPP': '공손 선어말 어미',\n",
       " 'EPT': '시제 선어말 어미',\n",
       " 'ET': '전성 어미',\n",
       " 'ETD': '관형형 전성 어미',\n",
       " 'ETN': '명사형 전성 어미',\n",
       " 'IC': '감탄사',\n",
       " 'JC': '접속 조사',\n",
       " 'JK': '조사',\n",
       " 'JKC': '보격 조사',\n",
       " 'JKG': '관형격 조사',\n",
       " 'JKI': '호격 조사',\n",
       " 'JKM': '부사격 조사',\n",
       " 'JKO': '목적격 조사',\n",
       " 'JKQ': '인용격 조사',\n",
       " 'JKS': '주격 조사',\n",
       " 'JX': '보조사',\n",
       " 'MA': '부사',\n",
       " 'MAC': '접속 부사',\n",
       " 'MAG': '일반 부사',\n",
       " 'MD': '관형사',\n",
       " 'MDN': '수 관형사',\n",
       " 'MDT': '일반 관형사',\n",
       " 'NN': '명사',\n",
       " 'NNB': '일반 의존 명사',\n",
       " 'NNG': '보통명사',\n",
       " 'NNM': '단위 의존 명사',\n",
       " 'NNP': '고유명사',\n",
       " 'NP': '대명사',\n",
       " 'NR': '수사',\n",
       " 'OH': '한자',\n",
       " 'OL': '외국어',\n",
       " 'ON': '숫자',\n",
       " 'SE': '줄임표',\n",
       " 'SF': '마침표, 물음표, 느낌표',\n",
       " 'SO': '붙임표(물결,숨김,빠짐)',\n",
       " 'SP': '쉼표,가운뎃점,콜론,빗금',\n",
       " 'SS': '따옴표,괄호표,줄표',\n",
       " 'SW': '기타기호 (논리수학기호,화폐기호)',\n",
       " 'UN': '명사추정범주',\n",
       " 'VA': '형용사',\n",
       " 'VC': '지정사',\n",
       " 'VCN': \"부정 지정사, 형용사 '아니다'\",\n",
       " 'VCP': \"긍정 지정사, 서술격 조사 '이다'\",\n",
       " 'VV': '동사',\n",
       " 'VX': '보조 용언',\n",
       " 'VXA': '보조 형용사',\n",
       " 'VXV': '보조 동사',\n",
       " 'XP': '접두사',\n",
       " 'XPN': '체언 접두사',\n",
       " 'XPV': '용언 접두사',\n",
       " 'XR': '어근',\n",
       " 'XSA': '형용사 파생 접미사',\n",
       " 'XSN': '명사파생 접미사',\n",
       " 'XSV': '동사 파생 접미사'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19cda6b7-d85c-42ee-a1a9-737c94a31c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'완전 가성비 최고 대용량 토너예요! 가격도 엄청 저렴한데 용량이 어마무시해서 스킨팩으로 아낌없이 쓰기에 너무 좋아요 성분도 크게 나쁜건 없는 것 같고 모찌 토너 이름답게 워터제형이지만 아주 수분감이 없지는 않고 촉촉한 편이예요 좀 가격대 있는 워터토너들은 스킨팩으로 막 쓰기엔 아까웠는데 이 제품은 가격이 너무 훌륭해서 신경쓰지 않고 막 써도 좋아요 '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f8485e60-76c2-41cf-9acc-ac6e80347e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('완전', 'NNG'),\n",
       " ('가', 'VV'),\n",
       " ('아', 'ECS'),\n",
       " ('성비', 'NNG'),\n",
       " ('최고', 'NNG'),\n",
       " ('대용량', 'NNG'),\n",
       " ('토너', 'NNG'),\n",
       " ('이', 'VCP'),\n",
       " ('에요', 'EFN'),\n",
       " ('!', 'SF'),\n",
       " ('가격', 'NNG'),\n",
       " ('도', 'JX'),\n",
       " ('엄청', 'MAG'),\n",
       " ('저렴', 'XR'),\n",
       " ('하', 'XSA'),\n",
       " ('ㄴ', 'ETD'),\n",
       " ('데', 'NNB'),\n",
       " ('용량', 'NNG'),\n",
       " ('이', 'JKS'),\n",
       " ('어', 'VV'),\n",
       " ('어', 'ECS'),\n",
       " ('마무', 'NNG'),\n",
       " ('시해', 'NNG'),\n",
       " ('서', 'JKM'),\n",
       " ('스킨', 'NNG'),\n",
       " ('팩', 'NNG'),\n",
       " ('으로', 'JKM'),\n",
       " ('아낌없이', 'MAG'),\n",
       " ('쓰', 'VV'),\n",
       " ('기에', 'ECD'),\n",
       " ('너무', 'MAG'),\n",
       " ('좋', 'VA'),\n",
       " ('아요', 'EFN'),\n",
       " ('성분', 'NNG'),\n",
       " ('도', 'JX'),\n",
       " ('크', 'VA'),\n",
       " ('게', 'ECD'),\n",
       " ('나쁘', 'VA'),\n",
       " ('ㄴ', 'ETD'),\n",
       " ('건', 'NNM'),\n",
       " ('없', 'VA'),\n",
       " ('는', 'ETD'),\n",
       " ('것', 'NNB'),\n",
       " ('같', 'VA'),\n",
       " ('고', 'ECE'),\n",
       " ('모', 'NNG'),\n",
       " ('찌', 'NNG'),\n",
       " ('토너', 'NNG'),\n",
       " ('이름', 'NNG'),\n",
       " ('답', 'XSA'),\n",
       " ('게', 'ECD'),\n",
       " ('워터', 'NNG'),\n",
       " ('제형', 'NNG'),\n",
       " ('이', 'VCP'),\n",
       " ('지만', 'ECE'),\n",
       " ('아주', 'MAG'),\n",
       " ('수분', 'NNG'),\n",
       " ('감이', 'NNG'),\n",
       " ('없', 'VA'),\n",
       " ('지', 'ECD'),\n",
       " ('는', 'JX'),\n",
       " ('않', 'VXV'),\n",
       " ('고', 'ECE'),\n",
       " ('촉촉', 'XR'),\n",
       " ('하', 'XSA'),\n",
       " ('ㄴ', 'ETD'),\n",
       " ('편', 'NNB'),\n",
       " ('이', 'VCP'),\n",
       " ('에요', 'EFN'),\n",
       " ('좀', 'MAG'),\n",
       " ('가격대', 'NNG'),\n",
       " ('있', 'VV'),\n",
       " ('는', 'ETD'),\n",
       " ('워터', 'NNG'),\n",
       " ('토너', 'NNG'),\n",
       " ('들', 'XSN'),\n",
       " ('은', 'JX'),\n",
       " ('스킨', 'NNG'),\n",
       " ('팩', 'NNG'),\n",
       " ('으로', 'JKM'),\n",
       " ('막', 'MAG'),\n",
       " ('쓰기', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('는', 'JX'),\n",
       " ('아깝', 'VA'),\n",
       " ('었', 'EPT'),\n",
       " ('는데', 'ECD'),\n",
       " ('이', 'MDT'),\n",
       " ('제품', 'NNG'),\n",
       " ('은', 'JX'),\n",
       " ('가격', 'NNG'),\n",
       " ('이', 'JKS'),\n",
       " ('너무', 'MAG'),\n",
       " ('훌륭', 'XR'),\n",
       " ('하', 'XSA'),\n",
       " ('어서', 'ECD'),\n",
       " ('신경', 'NNG'),\n",
       " ('쓰', 'VV'),\n",
       " ('지', 'ECD'),\n",
       " ('않', 'VXV'),\n",
       " ('고', 'ECE'),\n",
       " ('막', 'MAG'),\n",
       " ('쓰', 'VV'),\n",
       " ('어도', 'ECD'),\n",
       " ('좋', 'VA'),\n",
       " ('아요', 'EFN')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.pos(data['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0c92f747-6839-4eb5-8da0-d5ccdaecac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def myTokenizer(text) :\n",
    "#     # 불러온 문장을 인코딩 작업 utf-8 > euc-kr\n",
    "\n",
    "#     # 디코딩\n",
    "    \n",
    "#     d = pd.DataFrame(kkma.pos(text), columns = ['형태소', '품사'])\n",
    "#     d.set_index('품사', inplace = True)\n",
    "#     if ('VV' in d.index) | ('VA' in d.index) | ('NNG' in d.index) :\n",
    "#         return d.loc[d.index.intersection(['VV','VA','NNG']),'형태소'].values\n",
    "#     else :\n",
    "#         return []\n",
    "\n",
    "\n",
    "def myTokenizer(text):\n",
    "    # 형태소 분석\n",
    "    d = pd.DataFrame(kkma.pos(text), columns=['형태소', '품사'])\n",
    "    # 특정 품사만 추출\n",
    "    d_filtered = d[d['품사'].isin(['VV', 'VA', 'NNG'])]\n",
    "    return d_filtered['형태소'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5970aa05-a6f3-4ca7-a664-580c435364df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# test_token = TfidfVectorizer(tokenizer=myTokenizer)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# test_token.fit(data['review'])\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# test_token.vocabulary_\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# TF-IDF 벡터라이저 사용\u001b[39;00m\n\u001b[0;32m      6\u001b[0m test_token \u001b[38;5;241m=\u001b[39m TfidfVectorizer(tokenizer\u001b[38;5;241m=\u001b[39mmyTokenizer)\n\u001b[1;32m----> 7\u001b[0m test_token\u001b[38;5;241m.\u001b[39mfit(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 단어 사전 출력\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_token\u001b[38;5;241m.\u001b[39mvocabulary_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2103\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2096\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2098\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2099\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2100\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2101\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2102\u001b[0m )\n\u001b[1;32m-> 2103\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1274\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1276\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[76], line 16\u001b[0m, in \u001b[0;36mmyTokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmyTokenizer\u001b[39m(text):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# 형태소 분석\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     d \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(kkma\u001b[38;5;241m.\u001b[39mpos(text), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m형태소\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m품사\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# 특정 품사만 추출\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     d_filtered \u001b[38;5;241m=\u001b[39m d[d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m품사\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNNG\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\konlpy\\tag\\_kkma.py:81\u001b[0m, in \u001b[0;36mKkma.pos\u001b[1;34m(self, phrase, flatten, join)\u001b[0m\n\u001b[0;32m     79\u001b[0m             morphemes\u001b[38;5;241m.\u001b[39mappend(morpheme\u001b[38;5;241m.\u001b[39mgetString() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m morpheme\u001b[38;5;241m.\u001b[39mgetTag())\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m             morphemes\u001b[38;5;241m.\u001b[39mappend((morpheme\u001b[38;5;241m.\u001b[39mgetString(), morpheme\u001b[38;5;241m.\u001b[39mgetTag()))\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m join:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# test_token = TfidfVectorizer(tokenizer=myTokenizer)\n",
    "# test_token.fit(data['review'])\n",
    "# test_token.vocabulary_\n",
    "\n",
    "# TF-IDF 벡터라이저 사용\n",
    "test_token = TfidfVectorizer(tokenizer=myTokenizer)\n",
    "test_token.fit(data['review'])\n",
    "\n",
    "# 단어 사전 출력\n",
    "print(test_token.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2afec2c-5aa6-4333-b2b4-453a229550f8",
   "metadata": {},
   "source": [
    "### 꼬꼬마 인코딩에러 발생 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f595aac-b925-4397-931d-5728e8632382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kiwipiepy\n",
      "  Downloading kiwipiepy-0.18.0-cp311-cp311-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting kiwipiepy-model<0.19,>=0.18 (from kiwipiepy)\n",
      "  Downloading kiwipiepy_model-0.18.0.tar.gz (34.7 MB)\n",
      "     ---------------------------------------- 0.0/34.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/34.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.1/34.7 MB 1.8 MB/s eta 0:00:20\n",
      "     - -------------------------------------- 1.5/34.7 MB 13.6 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 7.2/34.7 MB 45.7 MB/s eta 0:00:01\n",
      "     ------------- ------------------------ 12.5/34.7 MB 110.0 MB/s eta 0:00:01\n",
      "     ------------------ -------------------- 16.4/34.7 MB 93.9 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 21.0/34.7 MB 93.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 25.8/34.7 MB 93.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 30.7/34.7 MB 108.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  34.7/34.7 MB 108.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  34.7/34.7 MB 108.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 34.7/34.7 MB 50.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from kiwipiepy) (1.26.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from kiwipiepy) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from tqdm->kiwipiepy) (0.4.6)\n",
      "Downloading kiwipiepy-0.18.0-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 72.9 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: kiwipiepy-model\n",
      "  Building wheel for kiwipiepy-model (setup.py): started\n",
      "  Building wheel for kiwipiepy-model (setup.py): finished with status 'done'\n",
      "  Created wheel for kiwipiepy-model: filename=kiwipiepy_model-0.18.0-py3-none-any.whl size=34843389 sha256=ed042eba4277a6d19f8133b2367698c32fc5d56ea7ea2c4901d6921979ef9baf\n",
      "  Stored in directory: c:\\users\\smhrd\\appdata\\local\\pip\\cache\\wheels\\1e\\c2\\c6\\68c479859c7e3b08e5644546b403a13d050195e2f21003ee31\n",
      "Successfully built kiwipiepy-model\n",
      "Installing collected packages: kiwipiepy-model, kiwipiepy\n",
      "Successfully installed kiwipiepy-0.18.0 kiwipiepy-model-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ced324f-9c01-404a-8464-1b3f5cf66ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi, basic_typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d3e4323-aa33-4b31-8668-6ad6422ffb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer2(text) : \n",
    "    text_space = Kiwi.space(text)\n",
    "    rs = Kiwi.tokenize(text_space, split_complex=True)\n",
    "    for token in rs :\n",
    "        if token.tag in ['VV','VA','NNG'] :\n",
    "            return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92fa2507-5f08-436c-be9f-47021127bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_test = data['review'][: 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d353349d-46bf-4669-82fd-a679526da9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review_idx\n",
       "0    완전 가성비 최고 대용량 토너예요! 가격도 엄청 저렴한데 용량이 어마무시해서 스킨팩...\n",
       "1    예전에 패드 너어어어무 만족해하며 썼던 기억이 떠올라서 구매해 본 토너입니다!! 아...\n",
       "2    이것민 바르고 화장한다는데 좋네요. 기존 토너보단 에멀젼 느낌이라 지성분들 이것만 ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7db7174d-6fc5-4768-9470-29bec5bd348e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Kiwi.space() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_token \u001b[38;5;241m=\u001b[39m TfidfVectorizer(tokenizer\u001b[38;5;241m=\u001b[39mmyTokenizer2)\n\u001b[1;32m----> 2\u001b[0m test_token\u001b[38;5;241m.\u001b[39mfit(test_test)\n\u001b[0;32m      3\u001b[0m test_token\u001b[38;5;241m.\u001b[39mvocabulary_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2103\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2096\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2098\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2099\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2100\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2101\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2102\u001b[0m )\n\u001b[1;32m-> 2103\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1274\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1276\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m, in \u001b[0;36mmyTokenizer2\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmyTokenizer2\u001b[39m(text) : \n\u001b[1;32m----> 2\u001b[0m     text_space \u001b[38;5;241m=\u001b[39m Kiwi\u001b[38;5;241m.\u001b[39mspace(text)\n\u001b[0;32m      3\u001b[0m     rs \u001b[38;5;241m=\u001b[39m Kiwi\u001b[38;5;241m.\u001b[39mtokenize(text_space, split_complex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m rs :\n",
      "\u001b[1;31mTypeError\u001b[0m: Kiwi.space() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "test_token = TfidfVectorizer(tokenizer=myTokenizer2)\n",
    "test_token.fit(test_test)\n",
    "test_token.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7b4ab2e-75b3-4964-8997-01080d036dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6700 entries, 0 to 6699\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   cos_name  6700 non-null   object\n",
      " 1   user_nm   6700 non-null   object\n",
      " 2   rating    6700 non-null   int64 \n",
      " 3   review    6700 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 519.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59edea-b1c6-4904-8226-7cdf23f61cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
